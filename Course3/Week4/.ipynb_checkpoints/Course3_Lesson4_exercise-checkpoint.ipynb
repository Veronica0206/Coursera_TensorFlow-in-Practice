{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "### YOUR CODE HERE\n",
    "# Figure out how to import regularizers\n",
    "from tensorflow.keras import regularizers\n",
    "###\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-14 20:17:12--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
      "Resolving storage.googleapis.com... 172.217.6.240\n",
      "Connecting to storage.googleapis.com|172.217.6.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93578 (91K) [text/plain]\n",
      "Saving to: '/tmp/sonnets.txt'\n",
      "\n",
      "/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2019-09-14 20:17:12 (1.89 MB/s) - '/tmp/sonnets.txt' saved [93578/93578]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "!/miniconda3/envs/tensorflow/bin/wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
    "    -O /tmp/sonnets.txt\n",
    "data = open(\"/tmp/sonnets.txt\").read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = \"pre\"))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x1272b7b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x1272b7b70>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x1272b7bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x1272b7bf8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1)) # Your Embedding Layer\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True))) # An LSTM Layer\n",
    "model.add(Dropout(0.2))  # A dropout layer\n",
    "model.add(LSTM(100)) # Another LSTM Layer\n",
    "model.add(Dense(total_words/2, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))) # A Dense Layer including regularizers\n",
    "model.add(Dense(total_words, activation = \"softmax\")) # A Dense Layer\n",
    "# Pick an optimizer\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"]) # Pick a loss function and an optimizer\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 33s 2ms/sample - loss: 6.9075 - accuracy: 0.0195\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 31s 2ms/sample - loss: 6.4977 - accuracy: 0.0224\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 36s 2ms/sample - loss: 6.4026 - accuracy: 0.0239\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 6.2800 - accuracy: 0.0296\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 6.1791 - accuracy: 0.0358\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 6.0969 - accuracy: 0.0391\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 6.0177 - accuracy: 0.0411\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 5.9333 - accuracy: 0.0452\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 5.8377 - accuracy: 0.0490\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 5.7352 - accuracy: 0.0572\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 5.6272 - accuracy: 0.0622\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 5.5271 - accuracy: 0.0678\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 5.4196 - accuracy: 0.0743\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 5.3141 - accuracy: 0.0820\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 5.2026 - accuracy: 0.0898\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 5.0998 - accuracy: 0.0960\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 4.9914 - accuracy: 0.1030\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 4.8878 - accuracy: 0.1127\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 4.7802 - accuracy: 0.1206\n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 4.6719 - accuracy: 0.1340\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 4.5692 - accuracy: 0.1453\n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 64s 4ms/sample - loss: 4.4680 - accuracy: 0.1544\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 65s 4ms/sample - loss: 4.3606 - accuracy: 0.1652\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 64s 4ms/sample - loss: 4.2571 - accuracy: 0.1795\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 4.1571 - accuracy: 0.1915\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 4.0515 - accuracy: 0.2075\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 3.9472 - accuracy: 0.2260\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 3.8464 - accuracy: 0.2378\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 3.7491 - accuracy: 0.2570\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 3.6466 - accuracy: 0.2711\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 3.5497 - accuracy: 0.2994\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 3.4594 - accuracy: 0.3163\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 49s 3ms/sample - loss: 3.3641 - accuracy: 0.3330\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 3.2790 - accuracy: 0.3520\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 3.1968 - accuracy: 0.3685\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 3.1154 - accuracy: 0.3895\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 3.0371 - accuracy: 0.4118\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 2.9611 - accuracy: 0.4254\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 2.8950 - accuracy: 0.4392\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 2.8294 - accuracy: 0.4523\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.7573 - accuracy: 0.4673\n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 2.6891 - accuracy: 0.4856\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 2.6317 - accuracy: 0.4968\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 2.5740 - accuracy: 0.5127\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 2.5213 - accuracy: 0.5233\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.4708 - accuracy: 0.5343\n",
      "Epoch 47/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 2.4097 - accuracy: 0.5433\n",
      "Epoch 48/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 2.3588 - accuracy: 0.5569\n",
      "Epoch 49/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.3122 - accuracy: 0.5660\n",
      "Epoch 50/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 2.2683 - accuracy: 0.5754\n",
      "Epoch 51/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 2.2228 - accuracy: 0.5906\n",
      "Epoch 52/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 2.1669 - accuracy: 0.6013\n",
      "Epoch 53/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 2.1307 - accuracy: 0.6088\n",
      "Epoch 54/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.0891 - accuracy: 0.6155\n",
      "Epoch 55/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 2.0447 - accuracy: 0.6246\n",
      "Epoch 56/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 2.0084 - accuracy: 0.6358\n",
      "Epoch 57/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 1.9712 - accuracy: 0.6444\n",
      "Epoch 58/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 1.9420 - accuracy: 0.6531\n",
      "Epoch 59/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.9054 - accuracy: 0.6605\n",
      "Epoch 60/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 1.8650 - accuracy: 0.6685\n",
      "Epoch 61/100\n",
      "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.8232 - accuracy: 0.6764\n",
      "Epoch 62/100\n",
      "15462/15462 [==============================] - 70s 5ms/sample - loss: 1.8244 - accuracy: 0.6707\n",
      "Epoch 63/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 1.7790 - accuracy: 0.6834\n",
      "Epoch 64/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 1.7483 - accuracy: 0.6898\n",
      "Epoch 65/100\n",
      "15462/15462 [==============================] - 65s 4ms/sample - loss: 1.7238 - accuracy: 0.6957\n",
      "Epoch 66/100\n",
      "15462/15462 [==============================] - 65s 4ms/sample - loss: 1.6922 - accuracy: 0.7031\n",
      "Epoch 67/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 1.6567 - accuracy: 0.7097\n",
      "Epoch 68/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 1.6349 - accuracy: 0.7155\n",
      "Epoch 69/100\n",
      "15462/15462 [==============================] - 75s 5ms/sample - loss: 1.6182 - accuracy: 0.7153\n",
      "Epoch 70/100\n",
      "15462/15462 [==============================] - 71s 5ms/sample - loss: 1.6036 - accuracy: 0.7196\n",
      "Epoch 71/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 1.5749 - accuracy: 0.7241\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462/15462 [==============================] - 76s 5ms/sample - loss: 1.5505 - accuracy: 0.7280\n",
      "Epoch 73/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 1.5258 - accuracy: 0.7317\n",
      "Epoch 74/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 1.5169 - accuracy: 0.7336\n",
      "Epoch 75/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 1.4938 - accuracy: 0.7403\n",
      "Epoch 76/100\n",
      "15462/15462 [==============================] - 68s 4ms/sample - loss: 1.4680 - accuracy: 0.7469\n",
      "Epoch 77/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 1.4488 - accuracy: 0.7479\n",
      "Epoch 78/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 1.4366 - accuracy: 0.7491\n",
      "Epoch 79/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 1.4097 - accuracy: 0.7553\n",
      "Epoch 80/100\n",
      "15462/15462 [==============================] - 67s 4ms/sample - loss: 1.3842 - accuracy: 0.7600\n",
      "Epoch 81/100\n",
      "15462/15462 [==============================] - 64s 4ms/sample - loss: 1.3831 - accuracy: 0.7599\n",
      "Epoch 82/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 1.3696 - accuracy: 0.7641\n",
      "Epoch 83/100\n",
      "15462/15462 [==============================] - 64s 4ms/sample - loss: 1.3507 - accuracy: 0.7681\n",
      "Epoch 84/100\n",
      "15462/15462 [==============================] - 70s 5ms/sample - loss: 1.3365 - accuracy: 0.7667\n",
      "Epoch 85/100\n",
      "15462/15462 [==============================] - 72s 5ms/sample - loss: 1.3154 - accuracy: 0.7734\n",
      "Epoch 86/100\n",
      "15462/15462 [==============================] - 67s 4ms/sample - loss: 1.3054 - accuracy: 0.7740\n",
      "Epoch 87/100\n",
      "15462/15462 [==============================] - 68s 4ms/sample - loss: 1.2834 - accuracy: 0.7796\n",
      "Epoch 88/100\n",
      "15462/15462 [==============================] - 71s 5ms/sample - loss: 1.2853 - accuracy: 0.7779\n",
      "Epoch 89/100\n",
      "15462/15462 [==============================] - 71s 5ms/sample - loss: 1.2624 - accuracy: 0.7802\n",
      "Epoch 90/100\n",
      "15462/15462 [==============================] - 74s 5ms/sample - loss: 1.2687 - accuracy: 0.7783\n",
      "Epoch 91/100\n",
      "15462/15462 [==============================] - 69s 4ms/sample - loss: 1.2345 - accuracy: 0.7887\n",
      "Epoch 92/100\n",
      "15462/15462 [==============================] - 69s 4ms/sample - loss: 1.2339 - accuracy: 0.7846\n",
      "Epoch 93/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 1.2203 - accuracy: 0.7891\n",
      "Epoch 94/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 1.2029 - accuracy: 0.7932\n",
      "Epoch 95/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 1.1952 - accuracy: 0.7919\n",
      "Epoch 96/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 1.1881 - accuracy: 0.7946\n",
      "Epoch 97/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 1.1725 - accuracy: 0.7967\n",
      "Epoch 98/100\n",
      "15462/15462 [==============================] - 70s 5ms/sample - loss: 1.1664 - accuracy: 0.7945\n",
      "Epoch 99/100\n",
      "15462/15462 [==============================] - 83s 5ms/sample - loss: 1.1502 - accuracy: 0.8020\n",
      "Epoch 100/100\n",
      "15462/15462 [==============================] - 86s 6ms/sample - loss: 1.1442 - accuracy: 0.8013\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, \"b\", label = \"Training accuracy\")\n",
    "plt.title(\"Training accuracy\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, \"b\", label = \"Training Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me Obi Wan Kenobi, you're my only hope some rehearse away away desire bear decay had outworn stol'n young back forth we grow leaves prove meetness cross cross feast forth decay heaven by trust fangled young new nearly feast by meetness meetness tend tend shade shines by them well well hath stol'n thy state with kind disdain had another meetness show me 'will hate hate hate mother lend meetness show your sight make held speaking heart after mother sinful place behind sweet heir state make heaven decay desire new dote dote mud hate's grow should do grow sad minds of the bath and lie to me die die\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding = \"pre\")\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
